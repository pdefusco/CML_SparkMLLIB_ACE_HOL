{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StandardScaler, Imputer, VectorSlicer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from sklearn import neighbors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"LC_Model_Pipeline\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('inferschema','true').csv('data/Data_Exploration.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.limit(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove categorical features that have too broad a set of values, or are highly imbalanced, or could cause data leakage. \n",
    "#We can elaborate and use them for feature extraction later, but they are not needed for a baseline\n",
    "remove = ['addr_state', 'earliest_cr_line', 'home_ownership', 'initial_list_status', 'issue_d', 'emp_length',\n",
    "          'loan_status', 'purpose', 'sub_grade', 'term', 'title', 'zip_code', 'application_type']\n",
    "df = df.drop(*remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list of categorical and numeric features\n",
    "cat_cols = [item[0] for item in df.dtypes if item[1].startswith('string')]\n",
    "num_cols = [item[0] for item in df.dtypes if item[1].startswith('in') or item[1].startswith('dou')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of nulls for each column:\n",
    "nulls = df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dti', 'int_rate', 'revol_util'], dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nulls.T[(nulls.T > 0).any(axis=1)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_list = list(nulls.T[(nulls.T > 0).any(axis=1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Both attributes are continuous so we will impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=impute_list, outputCols=[i+\"_imp\" for i in impute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = imputer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols+=[i+\"_imp\" for i in impute_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [i for i in num_cols if i not in impute_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols.remove(\"is_default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will choose these features for our baseline model:\n",
    "num_att, cat_att = num_cols, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the target feature from pipeline inputs\n",
    "if \"label\" in num_att:\n",
    "    num_att.remove(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages= []\n",
    "\n",
    "for col in cat_att:\n",
    "\n",
    "    stringIndexer = StringIndexer(inputCol = col , outputCol = col + '_StringIndex')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[col + '_ClassVect'])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "#Assembling mixed data type transformations:\n",
    "assemblerInputs = [c + \"_ClassVect\" for c in cat_att] + num_att\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "stages += [scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recreating the Chosen Logistic Regression model\n",
    "#Just load it from models dir\n",
    "lr = LogisticRegression(featuresCol='scaledFeatures', labelCol='is_default', maxIter=10, regParam=0.1, elasticNetParam=0.01)\n",
    "stages += [lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating and running the pipeline:\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice we did not split between train and test here - all we're looking for is a dry run of the model pipeline end to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.drop(\"features\", \"rawPrediction\", \"scaledFeatures\", \"grade_StringIndex\", \"grade_ClassVect\", \"verification_status_StringIndex\", \"verification_status_ClassVect\")\\\n",
    "    .toPandas().to_csv(\"data/chosen_model/predictions/data_w_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[acc_now_delinq: int, acc_open_past_24mths: int, annual_inc: double, avg_cur_bal: int, chargeoff_within_12_mths: int, collections_12_mths_ex_med: int, delinq_2yrs: int, delinq_amnt: int, dti: double, funded_amnt: int, funded_amnt_inv: double, grade: string, inq_last_6mths: int, installment: double, int_rate: double, loan_amnt: int, mo_sin_old_rev_tl_op: int, mo_sin_rcnt_rev_tl_op: int, mo_sin_rcnt_tl: int, mort_acc: int, num_accts_ever_120_pd: int, num_actv_bc_tl: int, num_actv_rev_tl: int, num_bc_sats: int, num_bc_tl: int, num_il_tl: int, num_op_rev_tl: int, num_rev_accts: double, num_rev_tl_bal_gt_0: int, num_sats: int, num_tl_30dpd: int, num_tl_90g_dpd_24m: int, num_tl_op_past_12m: int, open_acc: int, pct_tl_nvr_dlq: double, policy_code: int, pub_rec: int, pub_rec_bankruptcies: int, revol_bal: int, revol_util: double, tax_liens: int, tot_cur_bal: int, tot_hi_cred_lim: int, total_acc: int, total_bal_ex_mort: int, total_bc_limit: int, total_il_high_credit_limit: int, total_rev_hi_lim: int, verification_status: string, is_default: int, issue_month: int, dti_imp: double, int_rate_imp: double, revol_util_imp: double, grade_StringIndex: double, grade_ClassVect: vector, verification_status_StringIndex: double, verification_status_ClassVect: vector, features: vector, scaledFeatures: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(acc_now_delinq=0, acc_open_past_24mths=8, annual_inc=65000.0, avg_cur_bal=10086, chargeoff_within_12_mths=0, collections_12_mths_ex_med=0, delinq_2yrs=0, delinq_amnt=0, dti=23.84, funded_amnt=12000, funded_amnt_inv=12000.0, grade='B', inq_last_6mths=0, installment=253.79, int_rate=9.0, loan_amnt=12000, mo_sin_old_rev_tl_op=145, mo_sin_rcnt_rev_tl_op=4, mo_sin_rcnt_tl=1, mort_acc=2, num_accts_ever_120_pd=0, num_actv_bc_tl=2, num_actv_rev_tl=4, num_bc_sats=5, num_bc_tl=10, num_il_tl=13, num_op_rev_tl=11, num_rev_accts=22.0, num_rev_tl_bal_gt_0=4, num_sats=18, num_tl_30dpd=0, num_tl_90g_dpd_24m=0, num_tl_op_past_12m=3, open_acc=18, pct_tl_nvr_dlq=100.0, policy_code=1, pub_rec=0, pub_rec_bankruptcies=0, revol_bal=9786, revol_util=13.4, tax_liens=0, tot_cur_bal=181540, tot_hi_cred_lim=291626, total_acc=37, total_bal_ex_mort=74787, total_bc_limit=49500, total_il_high_credit_limit=105910, total_rev_hi_lim=73200, verification_status='Not Verified', is_default=0, issue_month=12, dti_imp=23.84, int_rate_imp=9.0, revol_util_imp=13.4, grade_StringIndex=0.0, grade_ClassVect=SparseVector(6, {0: 1.0}), verification_status_StringIndex=1.0, verification_status_ClassVect=SparseVector(2, {1: 1.0}), features=DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 8.0, 65000.0, 10086.0, 0.0, 0.0, 0.0, 0.0, 12000.0, 12000.0, 0.0, 253.79, 12000.0, 145.0, 4.0, 1.0, 2.0, 0.0, 2.0, 4.0, 5.0, 10.0, 13.0, 11.0, 22.0, 4.0, 18.0, 0.0, 0.0, 3.0, 18.0, 100.0, 1.0, 0.0, 0.0, 9786.0, 0.0, 181540.0, 291626.0, 37.0, 74787.0, 49500.0, 105910.0, 73200.0, 12.0, 23.84, 9.0, 13.4]), scaledFeatures=DenseVector([1.6476, -0.624, -0.4593, -0.4234, -0.3176, -0.168, -0.8687, 1.599, -0.0732, 0.9735, -0.1385, -0.175, -0.0775, -0.1291, -0.3625, -0.0209, -0.3416, -0.3411, -0.6468, -0.7434, -0.3416, -0.4268, -0.5494, -0.7876, 0.175, -0.3937, -0.7671, -0.5646, 0.0455, 0.3734, 0.5557, 0.513, 0.8378, -0.558, 1.0427, -0.0618, -0.1731, 0.3675, 1.0287, 0.6797, 0.0, -0.3799, -0.3546, -0.3184, -0.1518, 0.2753, 0.6923, 0.9302, 0.4512, 1.2862, 1.4043, 1.1376, 80.6406, 0.4866, -0.666, -1.6567]), rawPrediction=DenseVector([2.3114, -2.3114]), probability=DenseVector([0.9098, 0.0902]), prediction=0.0)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "listss = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 8.0, 65000.0, 10086.0, 0.0, 0.0, 0.0, 0.0, 12000.0, 12000.0, 0.0, 253.79, 12000.0, 145.0, 4.0, 1.0, 2.0, 0.0, 2.0, 4.0, 5.0, 10.0, 13.0, 11.0, 22.0, 4.0, 18.0, 0.0, 0.0, 3.0, 18.0, 100.0, 1.0, 0.0, 0.0, 9786.0, 0.0, 181540.0, 291626.0, 37.0, 74787.0, 49500.0, 105910.0, 73200.0, 12.0, 23.84, 9.0, 13.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 8.0, 65000.0, 10086.0, 0.0, 0.0, 0.0, 0.0, 12000.0, 12000.0, 0.0, 253.79, 12000.0, 145.0, 4.0, 1.0, 2.0, 0.0, 2.0, 4.0, 5.0, 10.0, 13.0, 11.0, 22.0, 4.0, 18.0, 0.0, 0.0, 3.0, 18.0, 100.0, 1.0, 0.0, 0.0, 9786.0, 0.0, 181540.0, 291626.0, 37.0, 74787.0, 49500.0]\n"
     ]
    }
   ],
   "source": [
    "print(listss[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipelineModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipelineModel.write().overwrite().save(\"data/chosen_model/pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4025.save.\n: org.apache.hadoop.fs.s3a.AWSClientIOException: doesBucketExist on cdp-sandbox-default-se.s3.us-east-2.amazonaws.com: com.amazonaws.SdkClientException: Unable to execute HTTP request: Certificate for <cdp-sandbox-default-se.s3.us-east-2.amazonaws.com.s3.amazonaws.com> doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]: Unable to execute HTTP request: Certificate for <cdp-sandbox-default-se.s3.us-east-2.amazonaws.com.s3.amazonaws.com> doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:204)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:112)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:315)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:407)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:311)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:286)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:506)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:387)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3344)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:138)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3404)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3372)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:493)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:699)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:180)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Certificate for <cdp-sandbox-default-se.s3.us-east-2.amazonaws.com.s3.amazonaws.com> doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1175)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1121)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920)\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5700)\n\tat com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5673)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4904)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1394)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1333)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:507)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:110)\n\t... 25 more\nCaused by: javax.net.ssl.SSLPeerUnverifiedException: Certificate for <cdp-sandbox-default-se.s3.us-east-2.amazonaws.com.s3.amazonaws.com> doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]\n\tat com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:467)\n\tat com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:397)\n\tat com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:355)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:373)\n\tat sun.reflect.GeneratedMethodAccessor345.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)\n\tat com.amazonaws.http.conn.$Proxy45.connect(Unknown Source)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:381)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:237)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1297)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n\t... 41 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-98a2b851786e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipelineModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://cdp-sandbox-default-se.s3.us-east-2.amazonaws.com/pdefusco\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/etc/spark/python/lib/pyspark.zip/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4025.save.\n: org.apache.hadoop.fs.s3a.AWSClientIOException: doesBucketExist on cdp-sandbox-default-se.s3.us-east-2.amazonaws.com: com.amazonaws.SdkClientException: Unable to execute HTTP request: Certificate for <cdp-sandbox-default-se.s3.us-east-2.amazonaws.com.s3.amazonaws.com> doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]: Unable to execute HTTP request: Certificate for <cdp-sandbox-default-se.s3.us-east-2.amazonaws.com.s3.amazonaws.com> doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:204)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:112)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:315)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:407)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:311)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:286)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:506)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:387)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3344)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:138)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3404)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3372)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:493)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:699)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:180)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Certificate for <cdp-sandbox-default-se.s3.us-east-2.amazonaws.com.s3.amazonaws.com> doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1175)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1121)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920)\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5700)\n\tat com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5673)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4904)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1394)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1333)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:507)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:110)\n\t... 25 more\nCaused by: javax.net.ssl.SSLPeerUnverifiedException: Certificate for <cdp-sandbox-default-se.s3.us-east-2.amazonaws.com.s3.amazonaws.com> doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]\n\tat com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:467)\n\tat com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:397)\n\tat com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:355)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:373)\n\tat sun.reflect.GeneratedMethodAccessor345.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)\n\tat com.amazonaws.http.conn.$Proxy45.connect(Unknown Source)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:381)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:237)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1297)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n\t... 41 more\n"
     ]
    }
   ],
   "source": [
    "pipelineModel.write().overwrite().save(\"s3a://cdp-sandbox-default-se.s3.us-east-2.amazonaws.com/pdefusco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

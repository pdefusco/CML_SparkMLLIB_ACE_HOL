{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from LC_Helper import vectorizerFunction, SmoteSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Baseline_Model\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",os.environ[\"STORAGE\"])\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = d8810fc1-63a7-4dd7-a956-fd4379df6765\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM default.lc_smote_subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list of categorical and numeric features\n",
    "num_cols = [item[0] for item in df.dtypes if item[1].startswith('in') or item[1].startswith('dou')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(['acc_now_delinq', 'acc_open_past_24mths', 'annual_inc', 'avg_cur_bal', 'funded_amnt', 'is_default'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.sampleBy(\"is_default\", fractions={0: 0.8, 1: 0.8}, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.subtract(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a Pipeline Object including One Hot Encoding of Categorical Features  \n",
    "def make_pipeline(spark_df):        \n",
    "     \n",
    "    for c in spark_df.columns:\n",
    "        spark_df = spark_df.withColumn(c, spark_df[c].cast(\"float\"))\n",
    "    \n",
    "    stages= []\n",
    "\n",
    "    cols = ['acc_now_delinq', 'acc_open_past_24mths', 'annual_inc', 'avg_cur_bal', 'funded_amnt']\n",
    "    \n",
    "    spark_df = spark_df.withColumn('acc_now_delinq',abs(spark_df.acc_now_delinq))\n",
    "    spark_df = spark_df.withColumn('acc_open_past_24mths',abs(spark_df.acc_open_past_24mths))\n",
    "    spark_df = spark_df.withColumn('annual_inc',abs(spark_df.annual_inc))\n",
    "    spark_df = spark_df.withColumn('avg_cur_bal',abs(spark_df.avg_cur_bal))\n",
    "    spark_df = spark_df.withColumn('funded_amnt',abs(spark_df.funded_amnt))\n",
    "    \n",
    "    #Assembling mixed data type transformations:\n",
    "    assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "    stages += [assembler]    \n",
    "    \n",
    "    #Scaling features\n",
    "    #scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    #stages += [scaler]\n",
    "    \n",
    "    \n",
    "    #RF Classifier\n",
    "    rf = LinearSVC(featuresCol='features', labelCol='is_default')\n",
    "    stages += [rf]\n",
    "    \n",
    "    #Creating and running the pipeline:\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipelineModel = pipeline.fit(spark_df)\n",
    "\n",
    "    return pipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipelineModel = make_pipeline(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in test.columns:\n",
    "    test = test.withColumn(c, test[c].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_now_delinq',\n",
       " 'acc_open_past_24mths',\n",
       " 'annual_inc',\n",
       " 'avg_cur_bal',\n",
       " 'funded_amnt',\n",
       " 'is_default',\n",
       " 'features',\n",
       " 'rawPrediction',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------+-----------+------------+----------+--------------------+--------------------+----------+\n",
      "|acc_now_delinq|acc_open_past_24mths| annual_inc|avg_cur_bal| funded_amnt|is_default|            features|       rawPrediction|prediction|\n",
      "+--------------+--------------------+-----------+-----------+------------+----------+--------------------+--------------------+----------+\n",
      "|   -0.08590768|            0.225552|-0.17482829|-0.34515885| 0.018119322|       1.0|[-0.0859076827764...|[-2.7969551340209...|       1.0|\n",
      "|   -0.08590768|           1.3480183|-0.02713315| -0.6537855|    0.640682|       1.0|[-0.0859076827764...|[-2.9291477748199...|       1.0|\n",
      "|   -0.08590768|          -1.1496115|-0.24321264| -0.7294536| -0.86209434|       0.0|[-0.0859076827764...|[-3.6664050970882...|       1.0|\n",
      "|   -0.08590768|         -0.23897669|0.056176037|-0.45687118|  0.31276748|       0.0|[-0.0859076827764...|[-1.9227331311467...|       1.0|\n",
      "|   -0.08590768|         -0.84606653|-0.08660933|  1.2570071|-0.039691083|       0.0|[-0.0859076827764...|[0.78210564586225...|       0.0|\n",
      "+--------------+--------------------+-----------+-----------+------------+----------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = df_model.rdd.map(lambda x: (x[\"is_default\"], x[\"prediction\"])) #, float(x[\"probability\"][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = spark.createDataFrame(input_data, [\"is_default\", \"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is_default', 'double'), ('prediction', 'double')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|   32|\n",
      "|       1.0|  219|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.select(\"is_default\", \"prediction\").groupby('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|is_default|count|\n",
      "+----------+-----+\n",
      "|       1.0|  105|\n",
      "|       0.0|  146|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.select(\"is_default\", \"prediction\").groupby('is_default').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"is_default\", rawPredictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "auroc = evaluator.evaluate(df_model, {evaluator.metricName: \"areaUnderROC\"})\n",
    "auprc = evaluator.evaluate(df_model, {evaluator.metricName: \"areaUnderPR\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC Curve: 0.5768\n",
      "Area under PR Curve: 0.4604\n"
     ]
    }
   ],
   "source": [
    "print(\"Area under ROC Curve: {:.4f}\".format(auroc))\n",
    "print(\"Area under PR Curve: {:.4f}\".format(auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy Spark Library \n",
    "# Creates instance of extended version of BinaryClassificationMetrics\n",
    "# using a DataFrame and its probability and label columns, as the output\n",
    "# from the classifier\n",
    "#bcm = BinaryClassificationMetrics(df_model, scoreCol='probability', labelCol='is_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can PLOT both ROC and PR curves!\n",
    "#fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "#bcm.plot_roc_curve(ax=axs[0])\n",
    "#bcm.plot_pr_curve(ax=axs[1])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.19      0.31       146\n",
      "         1.0       0.46      0.96      0.62       105\n",
      "\n",
      "    accuracy                           0.51       251\n",
      "   macro avg       0.67      0.58      0.47       251\n",
      "weighted avg       0.70      0.51      0.44       251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# And get the confusion matrix for any threshold we want\n",
    "y_true = df_model.select(['is_default']).collect()\n",
    "y_pred = df_model.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Saving pipeline to S3:\n",
    "pipelineModel.write().overwrite().save(os.environ[\"STORAGE\"]+\"/pdefusco/pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions\\\n",
    "  .write.format(\"parquet\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .saveAsTable(\n",
    "    'default.LC_predictions'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train\\\n",
    "  .write.format(\"parquet\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .saveAsTable(\n",
    "    'default.LC_train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test\\\n",
    "  .write.format(\"parquet\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .saveAsTable(\n",
    "    'default.LC_test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

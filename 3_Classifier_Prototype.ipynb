{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from LC_Helper import vectorizerFunction, SmoteSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Baseline_Model\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",os.environ[\"STORAGE\"])\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM default.lc_smote_subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list of categorical and numeric features\n",
    "num_cols = [item[0] for item in df.dtypes if item[1].startswith('in') or item[1].startswith('dou')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(['acc_now_delinq', 'acc_open_past_24mths', 'annual_inc', 'avg_cur_bal', 'funded_amnt', 'is_default'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.sampleBy(\"is_default\", fractions={0: 0.8, 1: 0.8}, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.subtract(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a Pipeline Object including One Hot Encoding of Categorical Features  \n",
    "def make_pipeline(spark_df):        \n",
    "     \n",
    "    for c in spark_df.columns:\n",
    "        spark_df = spark_df.withColumn(c, spark_df[c].cast(\"float\"))\n",
    "    \n",
    "    stages= []\n",
    "\n",
    "    cols = ['acc_now_delinq', 'acc_open_past_24mths', 'annual_inc', 'avg_cur_bal', 'funded_amnt']\n",
    "    \n",
    "    spark_df = spark_df.withColumn('acc_now_delinq',abs(spark_df.acc_now_delinq))\n",
    "    spark_df = spark_df.withColumn('acc_open_past_24mths',abs(spark_df.acc_open_past_24mths))\n",
    "    spark_df = spark_df.withColumn('annual_inc',abs(spark_df.annual_inc))\n",
    "    spark_df = spark_df.withColumn('avg_cur_bal',abs(spark_df.avg_cur_bal))\n",
    "    spark_df = spark_df.withColumn('funded_amnt',abs(spark_df.funded_amnt))\n",
    "    \n",
    "    #Assembling mixed data type transformations:\n",
    "    assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "    stages += [assembler]    \n",
    "    \n",
    "    #Scaling features\n",
    "    #scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    #stages += [scaler]\n",
    "    \n",
    "    \n",
    "    #RF Classifier\n",
    "    rf = LinearSVC(featuresCol='features', labelCol='is_default')\n",
    "    stages += [rf]\n",
    "    \n",
    "    #Creating and running the pipeline:\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipelineModel = pipeline.fit(spark_df)\n",
    "\n",
    "    return pipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipelineModel = make_pipeline(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in test.columns:\n",
    "    test = test.withColumn(c, test[c].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_now_delinq',\n",
       " 'acc_open_past_24mths',\n",
       " 'annual_inc',\n",
       " 'avg_cur_bal',\n",
       " 'funded_amnt',\n",
       " 'is_default',\n",
       " 'features',\n",
       " 'rawPrediction',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------+-----------+-----------+----------+--------------------+--------------------+----------+\n",
      "|acc_now_delinq|acc_open_past_24mths| annual_inc|avg_cur_bal|funded_amnt|is_default|            features|       rawPrediction|prediction|\n",
      "+--------------+--------------------+-----------+-----------+-----------+----------+--------------------+--------------------+----------+\n",
      "|   -0.07103118|         -0.44507217|-0.05017451| 0.98599064|-0.54676306|       1.0|[-0.0710311830043...|[0.97265596976254...|       0.0|\n",
      "|   -0.07103118|         -0.25531146| -0.0893869|  0.1979816|-0.87687695|       0.0|[-0.0710311830043...|[0.97265595623867...|       0.0|\n",
      "|   -0.07103118|          0.32007903|-0.10632743|  0.6991778|  0.5470389|       0.0|[-0.0710311830043...|[0.97265594036257...|       0.0|\n",
      "|   -0.07103118|          -1.1055471|-0.21852498|-0.67667013|-0.40859836|       1.0|[-0.0710311830043...|[0.97265584515078...|       0.0|\n",
      "|   -0.07103118|          -1.1183971|-0.21956149| -0.6567217| -0.5535294|       1.0|[-0.0710311830043...|[0.97265584613618...|       0.0|\n",
      "+--------------+--------------------+-----------+-----------+-----------+----------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = df_model.rdd.map(lambda x: (x[\"is_default\"], x[\"prediction\"])) #, float(x[\"probability\"][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = spark.createDataFrame(input_data, [\"is_default\", \"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is_default', 'double'), ('prediction', 'double')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|  294|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.select(\"is_default\", \"prediction\").groupby('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|is_default|count|\n",
      "+----------+-----+\n",
      "|       1.0|  107|\n",
      "|       0.0|  187|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.select(\"is_default\", \"prediction\").groupby('is_default').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"is_default\", rawPredictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "auroc = evaluator.evaluate(df_model, {evaluator.metricName: \"areaUnderROC\"})\n",
    "auprc = evaluator.evaluate(df_model, {evaluator.metricName: \"areaUnderPR\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC Curve: 0.5000\n",
      "Area under PR Curve: 0.3639\n"
     ]
    }
   ],
   "source": [
    "print(\"Area under ROC Curve: {:.4f}\".format(auroc))\n",
    "print(\"Area under PR Curve: {:.4f}\".format(auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy Spark Library \n",
    "# Creates instance of extended version of BinaryClassificationMetrics\n",
    "# using a DataFrame and its probability and label columns, as the output\n",
    "# from the classifier\n",
    "#bcm = BinaryClassificationMetrics(df_model, scoreCol='probability', labelCol='is_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can PLOT both ROC and PR curves!\n",
    "#fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "#bcm.plot_roc_curve(ax=axs[0])\n",
    "#bcm.plot_pr_curve(ax=axs[1])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      1.00      0.78       187\n",
      "         1.0       0.00      0.00      0.00       107\n",
      "\n",
      "    accuracy                           0.64       294\n",
      "   macro avg       0.32      0.50      0.39       294\n",
      "weighted avg       0.40      0.64      0.49       294\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/cdsw/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/cdsw/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# And get the confusion matrix for any threshold we want\n",
    "y_true = df_model.select(['is_default']).collect()\n",
    "y_pred = df_model.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Saving pipeline to S3:\n",
    "pipelineModel.write().overwrite().save(os.environ[\"STORAGE\"]+\"/pdefusco/pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions\\\n",
    "  .write.format(\"parquet\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .saveAsTable(\n",
    "    'default.LC_predictions'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train\\\n",
    "  .write.format(\"parquet\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .saveAsTable(\n",
    "    'default.LC_train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test\\\n",
    "  .write.format(\"parquet\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .saveAsTable(\n",
    "    'default.LC_test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
